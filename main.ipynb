{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "I used a convolutional neural network (CNN), made by KGP Talkie, to predict the labels of `test_time_series.csv`. I trained the model on accelerometer data from the WISDM dataset (link below), and evaluated the model's performance on the given `train_time_series.csv` and `train_labels.csv`. \n",
    "\n",
    "The reason I chose to use the WISDM dataset was that it had more samples than the given .csv files, and because when I trained it with WISDM I scored higher on the accuracy mark than when I trained it with the course-given training data. \n",
    "\n",
    "1. **KGP Talkie's Video**:\n",
    "    https://youtu.be/lUI6VMj43PE\n",
    "2. **WISDM Dataset**\n",
    "    http://www.cis.fordham.edu/wisdm/dataset.php"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time() # runtime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "\n",
    "The WISDM dataset's activity labels are: Walking, Jogging, Upstairs, Downstairs, Sitting, and Standing. The first thing I did was edit `train_time_series.csv` and `train_labels.csv` to be a merged pandas dataframe with column `activity` reverted back to label format (e.g., \"Walking\" instead of 2 etc.) to have a dataset similar to WISDM. I also filled in nan values for the `activity` column with their correct labels (since `train_labels.csv` labels were given every 10th observation). I also made edits to `test_time_series.csv`, and the merged train database (e.g., removing all columns except `x`, `y`, `z`, and `activity`).The merged database was saved as `edited_validation_database.csv` and the edited `test_time_series.csv` was saved as `edited_test_database.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def merge_processing(database):\n",
    "    del database['Unnamed: 0_y']\n",
    "    del database['UTC time_x']\n",
    "    del database['UTC time_y']\n",
    "    del database['accuracy']\n",
    "    del database['timestamp']\n",
    "    del database['Unnamed: 0_x']\n",
    "    database.columns = ['x', 'y', 'z', 'activity']\n",
    "    return database\n",
    "\n",
    "\n",
    "def activity_decoding(database):\n",
    "\n",
    "    def label_activity(row):\n",
    "        if row['activity'] == 1:\n",
    "            return 'Standing'\n",
    "        elif row['activity'] == 2:\n",
    "            return 'Walking'\n",
    "        elif row['activity'] == 3:\n",
    "            return 'Downstairs'\n",
    "        elif row['activity'] == 4:\n",
    "            return 'Upstairs'\n",
    "\n",
    "    database['activity'] = database.apply(lambda row: label_activity(row), axis=1)\n",
    "\n",
    "    return database\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # creating merged train CSV (i.e., train time-series + train labels)\n",
    "    train_labels = pd.read_csv('train_labels.csv')\n",
    "    train_time_series = pd.read_csv('train_time_series.csv')\n",
    "    merged_database = pd.merge(left=train_time_series, right=train_labels, how='left', left_on=\"timestamp\", right_on=\"timestamp\")\n",
    "    merged_database = merge_processing(merged_database)\n",
    "    #fillna used to turn nan values to correct activities\n",
    "    merged_database['activity'].fillna(method='backfill', inplace=True)\n",
    "    merged_database = activity_decoding(merged_database)\n",
    "\n",
    "    # Edited test CSV\n",
    "    test_time_series = pd.read_csv('test_time_series.csv')\n",
    "    del test_time_series['UTC time']\n",
    "    del test_time_series['accuracy']\n",
    "    del test_time_series['Unnamed: 0']\n",
    "    del test_time_series['timestamp']\n",
    "    test_time_series.columns = ['x', 'y', 'z']\n",
    "\n",
    "    # Saved data-frame objects as edited CSVs\n",
    "    merged_database.to_csv('edited_validation_database.csv')\n",
    "    test_time_series.to_csv('edited_test_database.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Data Pre-processing\n",
    "\n",
    "Edits to `WISDM_ar_v1.1_raw.txt`:\n",
    "1. Formatted to pandas dataframe\n",
    "2. Dropped timestamp & user-id column\n",
    "3. Balanced data (e.g., Walking, Standing etc have the same number of samples)\n",
    "4. Scaled x, y, z columns & converted to float \n",
    "6. Re-indexed \n",
    "7. Removed unwanted activities (Jogging, Sitting) that do not exist in course data\n",
    "8. Dropped every other row of database (since course data is taken at 10 Hz, and WISDM data at 20 Hz) \n",
    "\n",
    "Similar edits were made to `edited_validation_database.csv` and `edited_test_database.csv`. The edited WISDM database was saved as `scaled_train_database.csv`, the edited `edited_validation_database.csv` was saved as `scaled_validation_database.csv` and the edited `edited_test_database.csv` to `scaled_test_database.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error at line number:  281873\n",
      "Error at line number:  281874\n",
      "Error at line number:  281875\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# WISDM_formatting() taken from: https://youtu.be/lUI6VMj43PE?t=467\n",
    "def WISDM_formatting(csv_name): \n",
    "    type_file = open(csv_name)\n",
    "    lines = type_file.readlines()\n",
    "\n",
    "    Processed_List = []\n",
    "    for i, line in enumerate(lines):\n",
    "        try:\n",
    "            line = line.split(',')\n",
    "            last = line[5].split(';')[0]\n",
    "            last = last.strip()\n",
    "            if last == '':\n",
    "                break\n",
    "            temp = [line[0], line[1], line[2], line[3], line[4], last]\n",
    "            Processed_List.append(temp)\n",
    "        except:\n",
    "            print(\"Error at line number: \", i)\n",
    "\n",
    "    columns = ['user', 'activity', 'time', 'x', 'y', 'z']\n",
    "\n",
    "    edited_database = pd.DataFrame(data=Processed_List, columns=columns)\n",
    "    edited_database = edited_database.drop(['user', 'time'], axis=1).copy()\n",
    "\n",
    "    return edited_database\n",
    "\n",
    "# balance_scale_index() function \n",
    "# created using KGP Talkie: https://youtu.be/lUI6VMj43PE\n",
    "def balance_scale_index(database):\n",
    "    # defining scaling used\n",
    "    scale = StandardScaler()\n",
    "\n",
    "    # making 'x', 'y', 'z' columns float values\n",
    "    database['x'] = database['x'].astype('float')\n",
    "    database['y'] = database['y'].astype('float')\n",
    "    database['z'] = database['z'].astype('float')\n",
    "\n",
    "    if 'activity' in database.columns:\n",
    "        # determining number of samples of activity with least samples\n",
    "        # if database has 'activity' column\n",
    "        min_value = min(database['activity'].value_counts())\n",
    "\n",
    "        # taking min_value number of rows from each activity\n",
    "        Walking = database[database['activity'] == 'Walking'].head(min_value).copy()\n",
    "        Downstairs = database[database['activity'] == 'Downstairs'].head(min_value).copy()\n",
    "        Upstairs = database[database['activity'] == 'Upstairs'].head(min_value).copy()\n",
    "        Standing = database[database['activity'] == 'Standing'].head(min_value).copy()\n",
    "\n",
    "        # append balanced data\n",
    "        database = pd.DataFrame()\n",
    "        database = database.append([Walking, Downstairs, Upstairs, Standing])\n",
    "\n",
    "        # scale 'x', 'y', 'z' columns from balanced data and create pd.Dataframe()\n",
    "        x = database[['x', 'y', 'z']]\n",
    "        x = scale.fit_transform(x)\n",
    "        scaled_data = pd.DataFrame(data=x, columns=['x', 'y', 'z'])\n",
    "\n",
    "        # add activity column\n",
    "        y = database['activity']\n",
    "        scaled_data['activity'] = y.values\n",
    "\n",
    "    else:\n",
    "        # scale 'x', 'y', 'z' columns and create pd.Dataframe()\n",
    "        x = database[['x', 'y', 'z']]\n",
    "        x = scale.fit_transform(x)\n",
    "        scaled_data = pd.DataFrame(data=x, columns=['x', 'y', 'z'])\n",
    "\n",
    "    scaled_data.index = np.arange(0, len(scaled_data))\n",
    "\n",
    "    return scaled_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    WISDM = WISDM_formatting('WISDM_ar_v1.1_raw.txt')\n",
    "    WISDM = balance_scale_index(WISDM)\n",
    "    WISDM = WISDM.iloc[::2]\n",
    "\n",
    "    Validation = pd.read_csv('edited_validation_database.csv')\n",
    "    Validation = balance_scale_index(Validation)\n",
    "\n",
    "    Test = pd.read_csv('edited_test_database.csv')\n",
    "    Test = balance_scale_index(Test)\n",
    "\n",
    "    WISDM.to_csv(\"scaled_train_database.csv\")\n",
    "    Validation.to_csv(\"scaled_validation_database.csv\")\n",
    "    Test.to_csv(\"scaled_test_database.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions \n",
    "Here I defined some functions that I use later on. To feed data into a model it needs to be formatted a certain way, and `create_segments()` and `create_segments_and_labels()` format data appropriately (functions taken from KGP Talkie). Additionally, since a neural network cannot interpret labels such as \"Walking\" etc, this must be encoded into digits. For this I defined `encode_databases()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "import scipy.stats as stats\n",
    "import numpy as np\n",
    "\n",
    "# label encoder\n",
    "label = LabelEncoder()\n",
    "\n",
    "# encodes databases that have 'activity' columns\n",
    "def encode_databases(database1, database2):\n",
    "    database1['label'] = label.fit_transform(database1['activity'].values.ravel())\n",
    "    database2['label'] = label.transform(database2['activity'].values.ravel())\n",
    "    return database1, database2\n",
    "\n",
    "# function create_segments_and_labels() taken from Nils Ackermann\n",
    "# https://towardsdatascience.com/human-activity-recognition-har-tutorial-with-keras-and-core-ml-part-1-8c05e365dfa0\n",
    "def create_segments_and_labels(df, time_steps, step, label_name):\n",
    "    # x, y, z acceleration as features\n",
    "    n_features = 3\n",
    "    # Number of steps to advance in each iteration (for me, it should always\n",
    "    # be equal to the time_steps in order to have no overlap between segments)\n",
    "    # step = time_steps\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(0, len(df) - time_steps, step):\n",
    "        xs = df['x'].values[i: i + time_steps]\n",
    "        ys = df['y'].values[i: i + time_steps]\n",
    "        zs = df['z'].values[i: i + time_steps]\n",
    "        # retrieve most often used label in segment\n",
    "        label = stats.mode(df[label_name][i: i + time_steps])[0][0]\n",
    "        segments.append([xs, ys, zs])\n",
    "        labels.append(label)\n",
    "\n",
    "    # reshape segment & labels\n",
    "    reshaped_segments = np.asarray(segments, dtype=np.float32).reshape(-1, time_steps, n_features)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    return reshaped_segments, labels\n",
    "\n",
    "\n",
    "def create_segments(df, time_steps, step):\n",
    "    # x, y, z acceleration features\n",
    "    n_features = 3\n",
    "    \n",
    "    segments = []\n",
    "    for i in range(0, len(df) - time_steps, step):\n",
    "        xs = df['x'].values[i: i + time_steps]\n",
    "        ys = df['y'].values[i: i + time_steps]\n",
    "        zs = df['z'].values[i: i + time_steps]\n",
    "        segments.append([xs, ys, zs])\n",
    "\n",
    "    # reshape segment\n",
    "    reshaped_segments = np.asarray(segments, dtype=np.float32).reshape(-1, time_steps, n_features)\n",
    "\n",
    "    return reshaped_segments\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Model\n",
    "\n",
    "Now that all the functions are defined, I need to do the following:\n",
    "1. encode train_database (e.g., WISDM database) & validation_database (e.g., course training data) activity columns with `encode_databases()`.\n",
    "2. Format train_database into appropriate segments and labels, then train_test_split them (to have the model test itself with WISDM data). I'll also format validation_database into appropriate segments and labels to later test how well the model can predict the labels of course data. \n",
    "3. Reshape x_train, x_test, x_val into a shape that can be accepted by the model (4D Shape). \n",
    "4. Create the model (which is taken directly from KGP Talkie)\n",
    "5. Fit the model on x_train/y_train and evaluate its performance on x_val/y_val.\n",
    "\n",
    "As can be seen, the model has around 46-47% accuracy in predicting values which ... yikes! Isn't great. This could be due to not having enough data points. It could also be a result of taking data from two different datasets which have different environmental variables that are not controlled for. Two datasets of accelerometer data were used in order to have more data points - this was the preferred choice since using just one dataset led to decreased accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "Train on 568 samples, validate on 143 samples\n",
      "Epoch 1/15\n",
      "568/568 [==============================] - 0s 789us/sample - loss: 1.7027 - acc: 0.2482 - val_loss: 1.5140 - val_acc: 0.2797\n",
      "Epoch 2/15\n",
      "568/568 [==============================] - 0s 126us/sample - loss: 1.4442 - acc: 0.3222 - val_loss: 1.2625 - val_acc: 0.3846\n",
      "Epoch 3/15\n",
      "568/568 [==============================] - 0s 122us/sample - loss: 1.2543 - acc: 0.4401 - val_loss: 1.0556 - val_acc: 0.6853\n",
      "Epoch 4/15\n",
      "568/568 [==============================] - 0s 111us/sample - loss: 1.0773 - acc: 0.5933 - val_loss: 0.8696 - val_acc: 0.7133\n",
      "Epoch 5/15\n",
      "568/568 [==============================] - 0s 116us/sample - loss: 0.9471 - acc: 0.6109 - val_loss: 0.7158 - val_acc: 0.7413\n",
      "Epoch 6/15\n",
      "568/568 [==============================] - 0s 108us/sample - loss: 0.7721 - acc: 0.6761 - val_loss: 0.6007 - val_acc: 0.7692\n",
      "Epoch 7/15\n",
      "568/568 [==============================] - 0s 112us/sample - loss: 0.7107 - acc: 0.7095 - val_loss: 0.5583 - val_acc: 0.7832\n",
      "Epoch 8/15\n",
      "568/568 [==============================] - 0s 105us/sample - loss: 0.6448 - acc: 0.7377 - val_loss: 0.5167 - val_acc: 0.8042\n",
      "Epoch 9/15\n",
      "568/568 [==============================] - 0s 118us/sample - loss: 0.6113 - acc: 0.7271 - val_loss: 0.5132 - val_acc: 0.7832\n",
      "Epoch 10/15\n",
      "568/568 [==============================] - 0s 109us/sample - loss: 0.5668 - acc: 0.7623 - val_loss: 0.4808 - val_acc: 0.8042\n",
      "Epoch 11/15\n",
      "568/568 [==============================] - 0s 109us/sample - loss: 0.5672 - acc: 0.7658 - val_loss: 0.4942 - val_acc: 0.8042\n",
      "Epoch 12/15\n",
      "568/568 [==============================] - 0s 112us/sample - loss: 0.5059 - acc: 0.7958 - val_loss: 0.4598 - val_acc: 0.8252\n",
      "Epoch 13/15\n",
      "568/568 [==============================] - 0s 126us/sample - loss: 0.5030 - acc: 0.7782 - val_loss: 0.4693 - val_acc: 0.7972\n",
      "Epoch 14/15\n",
      "568/568 [==============================] - 0s 109us/sample - loss: 0.4813 - acc: 0.7870 - val_loss: 0.4390 - val_acc: 0.8252\n",
      "Epoch 15/15\n",
      "568/568 [==============================] - 0s 110us/sample - loss: 0.4495 - acc: 0.8081 - val_loss: 0.4411 - val_acc: 0.8112\n",
      "105/105 [==============================] - 0s 106us/sample - loss: 1.3456 - acc: 0.4476\n",
      "val_loss: 1.3456271589511917 accuracy: 0.44761905\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import scipy.stats as stats\n",
    "\n",
    "# defining label encoder\n",
    "label = LabelEncoder()\n",
    "\n",
    "# reading CSVs to pandas dataframe objects\n",
    "train_database = pd.read_csv(\"scaled_train_database.csv\")\n",
    "validation_database = pd.read_csv(\"scaled_validation_database.csv\")\n",
    "test_database = pd.read_csv(\"scaled_test_database.csv\")\n",
    "\n",
    "# encoding databases\n",
    "train_database, validation_database = encode_databases(train_database, validation_database)\n",
    "\n",
    "Fs = 10  # 10 b/c course data is in 10 Hz\n",
    "frame_size = 9  # number of rows to take for each prediction\n",
    "hop_size = 10  # number of rows to hope from one prediction to the next (determines overlap between frames)\n",
    "\n",
    "x_train, y_train = create_segments_and_labels(df=train_database,\n",
    "                                              time_steps=frame_size,\n",
    "                                              step=hop_size,\n",
    "                                              label_name='label'\n",
    "                                              )\n",
    "\n",
    "x_train, x_test, y_train, y_test = train_test_split(x_train,\n",
    "                                                    y_train,\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=0,\n",
    "                                                    stratify=y_train\n",
    "                                                    )\n",
    "\n",
    "x_val, y_val = create_segments_and_labels(df=validation_database,\n",
    "                                          time_steps=frame_size,\n",
    "                                          step=hop_size,\n",
    "                                          label_name='label'\n",
    "                                          )\n",
    "\n",
    "\n",
    "# reshaping: since model accepts 4D data, x_train x_test and x_val must be reshaped as follows:\n",
    "x_train = x_train.reshape(568, 9, 3, 1)\n",
    "x_test = x_test.reshape(143, 9, 3, 1)\n",
    "x_val = x_val.reshape(105, 9, 3, 1)\n",
    "\n",
    "# layering model (from KGP Talkie)\n",
    "model = Sequential()\n",
    "model.add(Conv2D(16, (2, 2), activation='relu'))\n",
    "model.add(Dropout(0.1))\n",
    "model.add(Conv2D(32, (2, 2), activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(6, activation=\"softmax\"))\n",
    "\n",
    "# compiling model\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss=\"sparse_categorical_crossentropy\", metrics=['accuracy'])\n",
    "\n",
    "# fitting model\n",
    "history = model.fit(x_train, y_train, epochs=15, validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "# evaluating model and print out performance\n",
    "scores = model.evaluate(x_val, y_val, verbose=1)\n",
    "print(\"val_loss:\", scores[0], \"accuracy:\", scores[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Test Data\n",
    "The test data must be formated from the `test_database` using `create_segments()`, and reshaped so it can be interpreted by the model. Then I use `model.predict_classes()` to predict the classes of the `x_test` segments. This is going to give me a list of integer representations of the classes, and these need to be converted to their labels with `label.inverse_transform()`. After they're converted to their label, they need to again converted to integers so that 'Standing' = 1, 'Walking'= 2 etc.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125/125 [==============================] - 0s 350us/sample\n",
      "There are 125 predictions.\n",
      "The predicted classes are: [4, 4, 4, 4, 4, 2, 4, 3, 2, 4, 4, 4, 3, 4, 4, 3, 4, 1, 4, 3, 4, 4, 1, 2, 4, 4, 4, 3, 3, 4, 4, 4, 2, 4, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 4, 2, 4, 4, 2, 4, 4, 4, 4, 4, 1, 3, 4, 3, 4, 4, 3, 4, 4, 2, 2, 2, 3, 4, 3, 3, 3, 2, 2, 4, 4, 3, 2, 2, 3, 2, 2, 2, 2, 2, 2, 3, 4, 3, 2, 2, 3, 2, 4, 3, 2, 4, 2, 3, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 2, 4, 3, 2, 2, 4, 2, 4]\n",
      "The runtime in seconds is: 7.755272388458252\n"
     ]
    }
   ],
   "source": [
    "# making test data segments\n",
    "x_test = create_segments(df=test_database,\n",
    "                         time_steps=frame_size,\n",
    "                         step=hop_size\n",
    "                         )\n",
    "\n",
    "# re-shaping test data\n",
    "x_test = x_test.reshape(125, 9, 3, 1)\n",
    "\n",
    "# predicting results\n",
    "program_encoded_class_results = model.predict_classes(x_test, verbose=1)\n",
    "label_class_result = label.inverse_transform(program_encoded_class_results)\n",
    "\n",
    "# results to be submitted\n",
    "predicted_labels_coded = []\n",
    "for i in range(len(label_class_result)):\n",
    "    if label_class_result[i] == 'Standing':\n",
    "        predicted_labels_coded.append(1)\n",
    "    elif label_class_result[i] == 'Walking':\n",
    "        predicted_labels_coded.append(2)\n",
    "    elif label_class_result[i] == 'Downstairs':\n",
    "        predicted_labels_coded.append(3)\n",
    "    elif label_class_result[i] == 'Upstairs':\n",
    "        predicted_labels_coded.append(4)\n",
    "\n",
    "print(\"There are\", len(predicted_labels_coded), \"predictions.\")\n",
    "print(\"The predicted classes are:\", predicted_labels_coded)\n",
    "\n",
    "end = time.time() # timer for runtime\n",
    "print(\"The runtime in seconds is:\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# appending predicted labels to test_labels.csv and overwriting test_labels.csv file\n",
    "test_labels = pd.read_csv(\"test_labels.csv\")\n",
    "test_labels['label'] = predicted_labels_coded\n",
    "test_labels.to_csv(\"test_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
